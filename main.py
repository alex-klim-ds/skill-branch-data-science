import numpy as np

#посчитать значений производной функции $\cos(x) + 0.05x^3 + \log_2{x^2}$
#в точке $x = 10$. Ответ округлить до 2-го знака.
#Пожалуйста назовите функцию derivation, функция должна принимать точку,
#в которой нужно вычислить значение производной, и функцию,
#производную которой мы хотим вычислить.
def func_1(x):
    return np.cos(x) + 0.05 * x**3 + np.log2(x**2)
def derivation(x):#производная от function_1(x)
    return - np.sin(x) + 0.15 * x**2 + \
           (2 / (x * np.log(2)))
print(round(derivation(10), 2))

#посчитать значение градиента функции
#x_1^2\cos(x_2) + 0.05x_2^3 + 3x_1^3\log_2{x_2^2}$ в точке $(10, 1)$.
#Пожалуйста назовите функцию gradient, функция должна принимать список
#с координатами точки, в которой нужно вычислить значение производной,
#и функцию, производную которой мы хотим вычислить.
#Ответ округлить до 2-го знака.
def func_2(x1, x2):
    return x1**2 * np.cos(x2) + 0.05 * x2**3 + \
           3 * x1**3 * np.log2(x2**2)
def d_dx1(x1, x2):#частная производная по x1
    return (round(9*x1**2 * np.log2(x2**2) + \
                 2 * x1 * np.cos(x2), 2))
def d_dx2(x1, x2):#частная производная по x2
    return (round(0.15 * x2**2 + ((6 * x1**3) / np.log(2) * (x2)) - \
                 x1**2 * np.sin(x2), 2))
def gradient(x1, x2):
    return ([d_dx1(x1, x2), d_dx2(x1, x2)])
print(gradient (10, 1))

#найти точку минимуму для функции $\cos(x) + 0.05x^3 + \log_2{x^2}$.
# Зафиксировать параметр $\epsilon = 0.001$, начальное значение
# принять равным 10. Выполнить 50 итераций градиентного спуска.
# Ответ округлить до второго знака; Пожалуйста назовите функцию
# gradient_optimization_one_dim. Функция должна принимать на вход функцию,
# которую требуется оптимизировать.

def gradient_optimization_one_dim (func_1, x= 10, max_iter = 50, eps = 0.001):
    while max_iter:
        max_iter -= 1
        xn = x - eps * derivation(x)
        x = xn
    return round(x, 2)
print(gradient_optimization_one_dim (func_1))

#найти точку минимуму для функции $x_1^2\cos(x_2) + 0.05x_2^3 + 3x_1^3\log_2{x_2^2}$.
#Зафиксировать параметр $\epsilon = 0.001$, начальные значения весов принять равным [4, 10].
#Выполнить 50 итераций градиентного спуска. Ответ округлить до второго знака;
#Пожалуйста назовите функцию gradient_optimization_multi_dim.